services:
  llama:
    build: .
    image: llama-cpp-sycl
    container_name: llama-sycl
    restart: unless-stopped
    devices:
      - /dev/dri:/dev/dri
    volumes:
      - /opt/llama.cpp/models:/models:ro
    ports:
      - "8080:8080"
    environment:
      - ZES_ENABLE_SYSMAN=1
    command: ["--model", "/models/${LLAMA_MODEL:?Set LLAMA_MODEL in .env (e.g. LLAMA_MODEL=Qwen3-4B-Q4_K_M.gguf)}"]
    tmpfs:
      - /tmp:size=512m
